# BeatGAN

### 概要
在本專題中使用的是對抗式神經網路模型，用Lakh Midi Dataset作為訓練資料集，其中包括 14,600多首不同歌曲。因此除了可以快速且大量的生成音樂外，使用者也可以通過選擇曲風來生成 自己所需要的音樂。 我們希望可以實現並優化現有的模型，使參數能達到更好的表現，並以此設計 一個平台，此平台可以達到兩種目標: 1.用網頁呈現出透過樂器間的生成器，讓使用者選擇曲風，隨 機生成各自的音軌，並整合每個音軌的參數，生成獨立的MIDI。 2.用一個樂器作為主要的生成動 機，接著以此動機將使用者所上傳的指定格式的midi檔音樂為基準聯合生成剩餘的樂器音軌，將音 樂常用的Jamming的概念用在伴奏生成上，進而生成曲風類似的伴奏

### 研究範圍
#### 訓練音樂生成模型

限制於資料集的特性，音樂性的部分較難掌控。主要是針對現行的模型架構、演算法進行實
作和優化改善，讓整體的客觀指標能夠有所進步，聽感上能夠更符合直覺。目標為讓DP、ISR上升 (現行演算法分別大約在50%、65%左右)、以及UPC、PR下降(現行演算法大約在45%左右)。從圖1 可以看到音樂訓練的過程，隨著訓練階段逐漸越來越進步，學習到音樂的規則。
<center> ![image](https://user-images.githubusercontent.com/61962782/197689841-0c7fedaa-f548-468c-a14f-be34cd7e473e.png) </center>

#### 合併樂曲自動產生伴奏
主要的輸入和輸出會是以平台的方式呈現，除了可以讓使用者選擇曲風來生成音樂，使用者也可以透過平台上傳樂曲，當接收到樂曲後，後端會將樂曲解析成npz文件，進而分析音符與樂曲進程的排列來生成相關的伴奏(e.g,和弦進程、鼓節奏、鋼琴音等)。
從圖2可以看到經過訓練後，生成的音樂架構是完整的且有邏輯性、結構。
<center>![image](https://user-images.githubusercontent.com/61962782/197690015-5fda1390-1974-41aa-8e6c-46b480b529f4.png)  </center>
            圖2 各樂器軌道音樂波形

### 成果
音樂生成的架構是以人類做音樂Jamming時的角度出發，給定一個特定軌道的序列 y ，藉由給定的音樂軌道序列 y 的生成剩餘的軌道完成歌曲段落。這樣音樂架構類似於隨機生成，不同的是，我們需要先將指定的輸入-音樂軌道序列 y 映射到空間向量 Z 中，並將向量 Z 與剩餘樂器中的隨機向量Zi同時作為輸入生成音樂，即Ｇi(Z,Zi)，以此達到類似於AI協同或輔助人類生成音樂伴奏。圖3為 Multi-Track Conditional GAN生成模型架構：

<img width="452" alt="image" src="https://user-images.githubusercontent.com/61962782/197690196-565f7ae5-1003-4bfc-99cf-a6a2b27cc126.png">


### 

## contributor
夏念愷 t107590019@ntut.org.tw
陳思齊 t107590025@ntut.org.tw
謝宗翰 t107590040@ntut.org.tw
