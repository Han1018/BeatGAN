{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MuseGan_predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGWLamOfcNguR34bte3vke",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Han1018/BeatGAN/blob/GenerateMidiAPI/Colab_API/MuseGan_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn093Wwj6bi7"
      },
      "source": [
        "### 導入雲端硬碟"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twq9ReH06bJC",
        "outputId": "a86fafee-cc2f-4cd2-b70e-c39f899afa12"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yCyy19h6keA"
      },
      "source": [
        "### 下載必備套件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5YCyssHc6hiQ",
        "outputId": "1aa56961-dc90-42ed-a998-0fc57e89fb52"
      },
      "source": [
        "!pip install pypianoroll==0.4.6\n",
        "!pip install tf-nightly\n",
        "!pip install gast==0.2.0\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypianoroll==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: pretty-midi<1.0,>=0.2.8 in /tensorflow-1.15.2/python3.7 (from pypianoroll==0.4.6) (0.2.8)\n",
            "Requirement already satisfied: six<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pypianoroll==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pypianoroll==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: scipy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pypianoroll==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: mido>=1.1.16 in /tensorflow-1.15.2/python3.7 (from pretty-midi<1.0,>=0.2.8->pypianoroll==0.4.6) (1.2.6)\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.7/dist-packages (2.8.0.dev20211001)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.21.0)\n",
            "Requirement already satisfied: tf-estimator-nightly~=2.7.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.7.0.dev2021092408)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.40.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=2.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.0)\n",
            "Requirement already satisfied: tb-nightly~=2.7.0.a in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.7.0a20211001)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.12.0)\n",
            "Collecting gast==0.4.0\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.37.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: keras-nightly~=2.7.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.7.0.dev2021100207)\n",
            "Requirement already satisfied: libclang~=11.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (11.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tf-nightly) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf-nightly) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf-nightly) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.7.0.a->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.7.0.a->tf-nightly) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf-nightly) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf-nightly) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.7.0.a->tf-nightly) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tb-nightly~=2.7.0.a->tf-nightly) (3.5.0)\n",
            "Installing collected packages: gast\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.2.0\n",
            "    Uninstalling gast-0.2.0:\n",
            "      Successfully uninstalled gast-0.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 1.15.2 requires gast==0.2.2, but you have gast 0.4.0 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gast==0.2.0\n",
            "  Using cached gast-0.2.0-py3-none-any.whl\n",
            "Installing collected packages: gast\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 1.15.2 requires gast==0.2.2, but you have gast 0.2.0 which is incompatible.\n",
            "tf-nightly 2.8.0.dev20211001 requires gast==0.4.0, but you have gast 0.2.0 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.7/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONCSUMnN6qzO"
      },
      "source": [
        "### 載入一代tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i8Ag9kM6of9",
        "outputId": "f779396a-5b5e-4168-c354-267778d4cb85"
      },
      "source": [
        "%tensorflow_version 1.10\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.10`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD3wXVa07QiF"
      },
      "source": [
        "### 更改路徑"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GqmY9Ix7PXc"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/museGan\") #更改路徑\n",
        "dir='/content/drive/MyDrive/museGan'\n",
        "#os.getcwd() #查看當前路徑"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xDEfsqo4hop"
      },
      "source": [
        "\"\"\"This script performs inference from a trained model.\"\"\"\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "from pprint import pformat\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import tensorflow as tf\n",
        "from config import LOGLEVEL, LOG_FORMAT\n",
        "from data_predict_version import load_data, get_samples\n",
        "from model import Model\n",
        "from utils import make_sure_path_exists, load_yaml, update_not_none\n",
        "LOGGER = logging.getLogger(\"musegan.inference\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUMCnSjU55Q4"
      },
      "source": [
        "def parse_arguments(folderName):\n",
        "\n",
        "    #設定以哪個資料夾下的Model做預測\n",
        "    # predict_folder='exp/default_2/'\n",
        "    predict_folder = folderName\n",
        "\n",
        "    \"\"\"Parse and return the command line arguments.\"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--result_dir',default=predict_folder,\n",
        "                        help=\"Directory where the results are saved.\")\n",
        "    parser.add_argument('--checkpoint_dir',default=predict_folder+'model/',\n",
        "                        help=\"Directory that contains checkpoints.\")\n",
        "    parser.add_argument('--params', '--params_file', '--params_file_path', default=predict_folder+'params.yaml',\n",
        "                        help=\"Path to the file that defines the \"\n",
        "                             \"hyperparameters.\")\n",
        "    parser.add_argument('--config',default=predict_folder+'config.yaml', help=\"Path to the configuration file.\")\n",
        "    parser.add_argument('--runs', type=int, default=\"1\",\n",
        "                        help=\"Times to run the inference process.\")\n",
        "    parser.add_argument('--rows', type=int, default=5,\n",
        "                        help=\"Number of images per row to be generated.\")\n",
        "    parser.add_argument('--columns', type=int, default=5,\n",
        "                        help=\"Number of images per column to be generated.\")\n",
        "    parser.add_argument('--lower', type=float, default=-2,\n",
        "                        help=\"Lower bound of the truncated normal random \"\n",
        "                             \"variables.\")\n",
        "    parser.add_argument('--upper', type=float, default=2,\n",
        "                        help=\"Upper bound of the truncated normal random \"\n",
        "                             \"variables.\")\n",
        "    parser.add_argument('--gpu', '--gpu_device_num', type=str, default=\"0\",\n",
        "                        help=\"The GPU device number to use.\")\n",
        "    #args = parser.parse_args()\n",
        "    # colab改成沒有輸入params，等等用指定的方式指定args\n",
        "    args = parser.parse_args(args=[])\n",
        "    return args"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxilHSwq57yo"
      },
      "source": [
        "def setup(folderName):\n",
        "    \"\"\"Parse command line arguments, load model parameters, load configurations\n",
        "    and setup environment.\"\"\"\n",
        "    # Parse the command line arguments\n",
        "    args = parse_arguments(folderName)\n",
        "\n",
        "    # Load parameters\n",
        "    # colab改成沒有輸入params，等等用指定的方式指定args\n",
        "    params = load_yaml(args.params)\n",
        "\n",
        "    # Load training configurations\n",
        "    config = load_yaml(args.config)\n",
        "    update_not_none(config, vars(args))\n",
        "\n",
        "    # Set unspecified schedule steps to default values\n",
        "    for target in (config['learning_rate_schedule'], config['slope_schedule']):\n",
        "        if target['start'] is None:\n",
        "            target['start'] = 0\n",
        "        if target['end'] is None:\n",
        "            target['end'] = config['steps']\n",
        "\n",
        "    # Make sure result directory exists\n",
        "    make_sure_path_exists(config['result_dir'])\n",
        "\n",
        "    # Setup GPUs\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = config['gpu']\n",
        "\n",
        "    return params, config"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XKNwrOi5-Ao"
      },
      "source": [
        "def inference_main(folderName):\n",
        "    \"\"\"Main function.\"\"\"\n",
        "    # Setup\n",
        "    logging.basicConfig(level=LOGLEVEL, format=LOG_FORMAT)\n",
        "    params, config = setup(folderName)\n",
        "    LOGGER.info(\"Using parameters:\\n%s\", pformat(params))\n",
        "    LOGGER.info(\"Using configurations:\\n%s\", pformat(config))\n",
        "\n",
        "    # ============================== Placeholders ==============================\n",
        "    placeholder_x = tf.placeholder(\n",
        "        tf.float32, shape=([None] + params['data_shape']))\n",
        "    placeholder_z = tf.placeholder(\n",
        "        tf.float32, shape=(None, params['latent_dim']))\n",
        "    placeholder_c = tf.placeholder(\n",
        "        tf.float32, shape=([None] + params['data_shape'][:-1] + [1]))\n",
        "    placeholder_suffix = tf.placeholder(tf.string)\n",
        "\n",
        "    # ================================= Model ==================================\n",
        "    # Create sampler configurations\n",
        "    sampler_config = {\n",
        "        'result_dir': config['result_dir'],\n",
        "        'image_grid': (config['rows'], config['columns']),\n",
        "        'suffix': placeholder_suffix, 'midi': config['midi'],\n",
        "        'colormap': np.array(config['colormap']).T,\n",
        "        'collect_save_arrays_op': config['save_array_samples'],\n",
        "        'collect_save_images_op': config['save_image_samples'],\n",
        "        'collect_save_pianorolls_op': config['save_pianoroll_samples']}\n",
        "\n",
        "    # Build model\n",
        "    model = Model(params)\n",
        "    if params.get('is_accompaniment'):\n",
        "        _ = model(\n",
        "            x=placeholder_x, c=placeholder_c, z=placeholder_z, mode='train',\n",
        "            params=params, config=config)\n",
        "        predict_nodes = model(\n",
        "            c=placeholder_c, z=placeholder_z, mode='predict', params=params,\n",
        "            config=sampler_config)\n",
        "    else:\n",
        "        _ = model(\n",
        "            x=placeholder_x, z=placeholder_z, mode='train', params=params,\n",
        "            config=config)\n",
        "        predict_nodes = model(\n",
        "            z=placeholder_z, mode='predict', params=params,\n",
        "            config=sampler_config)\n",
        "\n",
        "    # Get sampler op\n",
        "    sampler_op = tf.group([\n",
        "        predict_nodes[key] for key in (\n",
        "            'save_arrays_op', 'save_images_op', 'save_pianorolls_op')\n",
        "        if key in predict_nodes])\n",
        "\n",
        "    # ================================== Data ==================================\n",
        "    if params.get('is_accompaniment'):\n",
        "        data = load_data(config['data_source'], config['data_filename'])\n",
        "        print('predicted_data_issssss: ',data.shape)\n",
        "\n",
        "    # ========================== Session Preparation ===========================\n",
        "    # Get tensorflow session config\n",
        "    tf_config = tf.ConfigProto()\n",
        "    tf_config.gpu_options.allow_growth = True\n",
        "\n",
        "    # Create saver to restore variables\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    # =========================== Tensorflow Session ===========================\n",
        "    with tf.Session(config=tf_config) as sess:\n",
        "\n",
        "        # Restore the latest checkpoint\n",
        "        LOGGER.info(\"Restoring the latest checkpoint.\")\n",
        "        with open(os.path.join(config['checkpoint_dir'], 'checkpoint')) as f:\n",
        "            checkpoint_name = os.path.basename(\n",
        "                f.readline().split()[1].strip('\"'))\n",
        "        checkpoint_path = os.path.realpath(\n",
        "            os.path.join(config['checkpoint_dir'], checkpoint_name))\n",
        "        print('checkpoint_path is:::::',checkpoint_path)\n",
        "        saver.restore(sess, checkpoint_path)\n",
        "\n",
        "        # Run sampler op\n",
        "        for i in range(config['runs']):\n",
        "            feed_dict_sampler = {\n",
        "                placeholder_z: scipy.stats.truncnorm.rvs(\n",
        "                    config['lower'], config['upper'], size=(\n",
        "                        (config['rows'] * config['columns']),\n",
        "                        params['latent_dim'])),\n",
        "                placeholder_suffix: str(i)}  \n",
        "            if params.get('is_accompaniment'):\n",
        "                # sample_x = get_samples(\n",
        "                #     (config['rows'] * config['columns']), data,\n",
        "                #     use_random_transpose=config['use_random_transpose'])\n",
        "                # feed_dict_sampler[placeholder_c] = np.expand_dims(\n",
        "                #     sample_x[..., params['condition_track_idx']], -1)\n",
        "                feed_dict_sampler[placeholder_c]=data\n",
        "                # code in \"if(i<1)\" all are added py paul\n",
        "                if(i<1):\n",
        "                    tmp = feed_dict_sampler[placeholder_c]\n",
        "                    print(i,tmp.shape)\n",
        "                    for j in range(24):  \n",
        "                        print(feed_dict_sampler[placeholder_c].shape)  \n",
        "                        feed_dict_sampler[placeholder_c]=np.vstack([feed_dict_sampler[placeholder_c],tmp])                 \n",
        "                    \t\n",
        "                    #print('before:\\n',sample_x.shape,'\\nAfter:\\n', sample_x[..., params['condition_track_idx']].shape)\n",
        "                    #print('params[\\'condition_track_idx\\']:',params['condition_track_idx'])\n",
        "                    print('feed_dict_sampler:\\n',feed_dict_sampler[placeholder_c].shape)\n",
        "            sess.run(sampler_op, feed_dict=feed_dict_sampler)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7xl9ko8Sh6D"
      },
      "source": [
        "from pypianoroll import Multitrack\n",
        "\n",
        "def get_midi(route):\n",
        "  midi_file = Multitrack(route+'fake_x_bernoulli_sampling_0.npz')\n",
        "  midi_file.write(route+'fake_x_0.mid')\n",
        "  return midi_file"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I11r6ouGVUwJ"
      },
      "source": [
        "\n",
        "def predict_trackCondition():\n",
        "  folderName = 'exp/accompaniment/piano/'\n",
        "  inference_main(folderName)\n",
        "\n",
        "def predict_Normal(genre):\n",
        "\n",
        "  folderName = 'exp/'+genre+'/'\n",
        "  inference_main(folderName)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFCwxCkdmhGh"
      },
      "source": [
        "def get_genre(genre):\n",
        "  if(genre is 0):\n",
        "    return 'normal_pop'\n",
        "  if(genre is 1):\n",
        "    return 'default_2'\n",
        "  if(genre is 2):\n",
        "    return 'normal_electronic'\n",
        "  if(genre is 3):\n",
        "    return 'normal_origin'"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOD0zA8RXMDX"
      },
      "source": [
        "# API 拿midi檔\n",
        "### flask & ngrok source : https://aishuafei.com/google-colab-flask/\n",
        "### midi:https://stackoverflow.com/questions/28121776/how-do-i-allow-users-to-download-a-midi-file-with-flask-without-getting-a-0-byte\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDyzwYDtTgXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb7c205-a596-4488-c02c-9fcf0700ebd8"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask,send_file,request\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "\n",
        "@app.route(\"/trackCondition\")\n",
        "def trackCondition():\n",
        "  base='/content/drive/MyDrive/museGan/exp/accompaniment/piano/pianorolls/fake_x_bernoulli_sampling/'\n",
        "  predict_trackCondition()\n",
        "  midi = get_midi(base)\n",
        "  new_file = open(base+'fake_x_0.mid', 'rb')\n",
        "  return send_file(new_file, mimetype='audio/midi')\n",
        "  \n",
        "@app.route(\"/random\")\n",
        "def ramdom():\n",
        "  # base param\n",
        "  base ='/content/drive/MyDrive/museGan/exp/'\n",
        "  lastParam = '/pianorolls/fake_x_bernoulli_sampling/'\n",
        "\n",
        "  #request data\n",
        "  rJson = request.json\n",
        "  genre_num = rJson['genre']#0,1,2\n",
        "  \n",
        "  #print('\\ngenre_num : ',genre_num,'\\n')\n",
        "  \n",
        "  genre = get_genre(genre_num)\n",
        "  \n",
        "  predict_Normal(genre)\n",
        "  midi = get_midi(base+genre+lastParam)\n",
        "  new_file = open(base+genre+lastParam+'fake_x_0.mid', 'rb')\n",
        "  return send_file(new_file, mimetype='audio/midi')\n",
        "  \n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
            "werkzeug             INFO      * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://aba2-35-236-131-212.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "musegan.inference    INFO     Using parameters:\n",
            "{'beat_resolution': 12,\n",
            " 'condition_track_idx': None,\n",
            " 'data_shape': [4, 48, 84, 5],\n",
            " 'is_accompaniment': False,\n",
            " 'is_conditional': False,\n",
            " 'latent_dim': 128,\n",
            " 'nets': {'discriminator': 'default', 'generator': 'default'},\n",
            " 'use_binary_neurons': False}\n",
            "musegan.inference    INFO     Using configurations:\n",
            "{'adam': {'beta1': 0.5, 'beta2': 0.9},\n",
            " 'batch_size': 64,\n",
            " 'checkpoint_dir': 'exp/normal_electronic/model/',\n",
            " 'colormap': [[1.0, 0.0, 0.0],\n",
            "              [1.0, 0.5, 0.0],\n",
            "              [0.0, 1.0, 0.0],\n",
            "              [0.0, 0.0, 1.0],\n",
            "              [0.0, 0.5, 1.0]],\n",
            " 'columns': 5,\n",
            " 'config': 'exp/normal_electronic/config.yaml',\n",
            " 'data_filename': 'train_x_lpd_5_phr',\n",
            " 'data_root': None,\n",
            " 'data_source': 'sa',\n",
            " 'evaluate_steps': 100,\n",
            " 'gan_loss_type': 'wasserstein',\n",
            " 'gpu': '0',\n",
            " 'initial_learning_rate': 0.001,\n",
            " 'learning_rate_schedule': {'end': 50000, 'end_value': 0.0, 'start': 45000},\n",
            " 'log_loss_steps': 100,\n",
            " 'lower': -2,\n",
            " 'midi': {'is_drums': [1, 0, 0, 0, 0],\n",
            "          'lowest_pitch': 24,\n",
            "          'programs': [0, 0, 25, 33, 48],\n",
            "          'tempo': 100},\n",
            " 'n_dis_updates_per_gen_update': 5,\n",
            " 'n_jobs': 20,\n",
            " 'params': 'exp/normal_electronic/params.yaml',\n",
            " 'result_dir': 'exp/normal_electronic/',\n",
            " 'rows': 5,\n",
            " 'runs': 1,\n",
            " 'sample_grid': [8, 8],\n",
            " 'save_array_samples': True,\n",
            " 'save_checkpoint_steps': 10000,\n",
            " 'save_image_samples': True,\n",
            " 'save_pianoroll_samples': True,\n",
            " 'save_samples_steps': 100,\n",
            " 'save_summaries_steps': 0,\n",
            " 'slope_schedule': {'end': 50000, 'end_value': 5.0, 'start': 10000},\n",
            " 'steps': 50000,\n",
            " 'upper': 2,\n",
            " 'use_gradient_penalties': True,\n",
            " 'use_learning_rate_decay': True,\n",
            " 'use_random_transpose': False,\n",
            " 'use_slope_annealing': False,\n",
            " 'use_train_test_split': False}\n",
            "model                INFO     Building model.\n",
            "model                INFO     Building training nodes.\n",
            "model                INFO     Building losses.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of true data:  (?, 4, 48, 84, 5)\n",
            "Shape of fake data:  (?, 4, 48, 84, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model                INFO     Building training ops.\n",
            "model                INFO     Building summaries.\n",
            "model                INFO     Building prediction nodes.\n",
            "musegan.inference    INFO     Restoring the latest checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint_path is::::: /content/drive/My Drive/museGan/exp/normal_electronic/model/model.ckpt-60000\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/museGan/exp/normal_electronic/model/model.ckpt-60000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tensorflow           INFO     Restoring parameters from /content/drive/My Drive/museGan/exp/normal_electronic/model/model.ckpt-60000\n",
            "127.0.0.1 - - [02/Oct/2021 09:26:30] \"\u001b[37mGET /random HTTP/1.1\u001b[0m\" 200 -\n",
            "werkzeug             INFO     127.0.0.1 - - [02/Oct/2021 09:26:30] \"\u001b[37mGET /random HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [02/Oct/2021 09:27:10] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
            "werkzeug             INFO     127.0.0.1 - - [02/Oct/2021 09:27:10] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [02/Oct/2021 09:27:10] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "werkzeug             INFO     127.0.0.1 - - [02/Oct/2021 09:27:10] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "musegan.inference    INFO     Using parameters:\n",
            "{'beat_resolution': 12,\n",
            " 'condition_track_idx': None,\n",
            " 'data_shape': [4, 48, 84, 5],\n",
            " 'is_accompaniment': False,\n",
            " 'is_conditional': False,\n",
            " 'latent_dim': 128,\n",
            " 'nets': {'discriminator': 'default', 'generator': 'default'},\n",
            " 'use_binary_neurons': False}\n",
            "musegan.inference    INFO     Using configurations:\n",
            "{'adam': {'beta1': 0.5, 'beta2': 0.9},\n",
            " 'batch_size': 64,\n",
            " 'checkpoint_dir': 'exp/default_2/model/',\n",
            " 'colormap': [[1.0, 0.0, 0.0],\n",
            "              [1.0, 0.5, 0.0],\n",
            "              [0.0, 1.0, 0.0],\n",
            "              [0.0, 0.0, 1.0],\n",
            "              [0.0, 0.5, 1.0]],\n",
            " 'columns': 5,\n",
            " 'config': 'exp/default_2/config.yaml',\n",
            " 'data_filename': 'train_x_lpd_5_phr',\n",
            " 'data_root': None,\n",
            " 'data_source': 'sa',\n",
            " 'evaluate_steps': 100,\n",
            " 'gan_loss_type': 'wasserstein',\n",
            " 'gpu': '0',\n",
            " 'initial_learning_rate': 0.001,\n",
            " 'learning_rate_schedule': {'end': 50000, 'end_value': 0.0, 'start': 45000},\n",
            " 'log_loss_steps': 100,\n",
            " 'lower': -2,\n",
            " 'midi': {'is_drums': [1, 0, 0, 0, 0],\n",
            "          'lowest_pitch': 24,\n",
            "          'programs': [0, 0, 25, 33, 48],\n",
            "          'tempo': 100},\n",
            " 'n_dis_updates_per_gen_update': 5,\n",
            " 'n_jobs': 20,\n",
            " 'params': 'exp/default_2/params.yaml',\n",
            " 'result_dir': 'exp/default_2/',\n",
            " 'rows': 5,\n",
            " 'runs': 1,\n",
            " 'sample_grid': [8, 8],\n",
            " 'save_array_samples': True,\n",
            " 'save_checkpoint_steps': 10000,\n",
            " 'save_image_samples': True,\n",
            " 'save_pianoroll_samples': True,\n",
            " 'save_samples_steps': 100,\n",
            " 'save_summaries_steps': 0,\n",
            " 'slope_schedule': {'end': 50000, 'end_value': 5.0, 'start': 10000},\n",
            " 'steps': 50000,\n",
            " 'upper': 2,\n",
            " 'use_gradient_penalties': True,\n",
            " 'use_learning_rate_decay': True,\n",
            " 'use_random_transpose': False,\n",
            " 'use_slope_annealing': False,\n",
            " 'use_train_test_split': False}\n",
            "model                INFO     Building model.\n",
            "model                INFO     Building training nodes.\n",
            "model                INFO     Building losses.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of true data:  (?, 4, 48, 84, 5)\n",
            "Shape of fake data:  (?, 4, 48, 84, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model                INFO     Building training ops.\n",
            "model                INFO     Building summaries.\n",
            "model                INFO     Building prediction nodes.\n",
            "musegan.inference    INFO     Restoring the latest checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint_path is::::: /content/drive/MyDrive/museGan/exp/default_2/model/model.ckpt-240000\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/museGan/exp/default_2/model/model.ckpt-240000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tensorflow           INFO     Restoring parameters from /content/drive/MyDrive/museGan/exp/default_2/model/model.ckpt-240000\n",
            "127.0.0.1 - - [02/Oct/2021 09:27:49] \"\u001b[37mGET /random HTTP/1.1\u001b[0m\" 200 -\n",
            "werkzeug             INFO     127.0.0.1 - - [02/Oct/2021 09:27:49] \"\u001b[37mGET /random HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        }
      ]
    }
  ]
}